# Physical AI Hack 2026 - Raw Archive

**Event**: Physical AI Hacks 2026  
**Dates**: January 31 – February 1, 2026 (48 hours)  
**Location**: Founders Inc, 2 Marina Blvd B300, San Francisco, CA 94123  
**Website**: https://physicalaihack.com/

---

## Table of Contents

1. [URLs & Links](#urls--links)
2. [Personal Recollection](#personal-recollection)
3. [Event Overview](#event-overview)
4. [Judging Protocol](#judging-protocol)
5. [Track Specifications](#track-specifications)
6. [Robots Available](#robots-available)
7. [VLA Models](#vla-models)
8. [GPU & Cloud Resources](#gpu--cloud-resources)
9. [Sponsors & Provided Resources](#sponsors--provided-resources)
10. [Technical Reference Guide](#technical-reference-guide)
11. [Luma Event Blasts](#luma-event-blasts)
12. [Velda Documentation](#velda-documentation)
13. [Team Submission Materials](#team-submission-materials)

---

## URLs & Links

### Event & Registration
- Event Website: https://physicalaihack.com/
- Luma Event Page: (Physical AI Hack 2026 on Luma, presented by Solo Tech)
- Robot Reservation Form: https://forms.gle/tKR8pNUdC7PJZRuK9
- Logistics Session (Google Meet): meet.google.com/zkt-zyjs-ueg

### Frameworks & Documentation
- LeRobot GitHub: https://github.com/huggingface/lerobot
- LeRobot Docs: https://huggingface.co/docs/lerobot
- SO-101 Official Docs: https://huggingface.co/docs/lerobot/en/so101
- SO-101 Wiki (Seeed): https://wiki.seeedstudio.com/lerobot_so100m/
- Koch Docs: https://huggingface.co/docs/lerobot/en/koch
- LeKiwi Docs: https://huggingface.co/docs/lerobot/en/lekiwi
- LeKiwi Wiki (Seeed): https://wiki.seeedstudio.com/lerobot_lekiwi/
- SO-ARM100 GitHub: https://github.com/TheRobotStudio/SO-ARM100/blob/main/README.md
- LeRobot Camera Code: https://github.com/huggingface/lerobot/blob/aec1b29d230341721c8d8f4413c47dcdb424cf5d/src/lerobot/cameras/opencv/camera_opencv.py

### Solo Tech
- Solo Tech Website: https://www.getsolo.tech/
- Solo-CLI GitHub: https://github.com/GetSoloTech/solo-cli
- Solo-CLI Docs: https://github.com/GetSoloTech/solo-cli/blob/main/solo/commands/robots/lerobot/README.md
- Solo Tech Discord: https://discord.gg/8kR5VvATUq
- Solo Tech HuggingFace: https://huggingface.co/GetSoloTech
- Solo Server GitHub: https://github.com/GetSoloTech/solo-server

### GPU & Cloud
- Velda (Hackathon): https://physical-ai.velda.cloud
- Velda Public: https://velda.cloud
- Velda Docs: https://docs.velda.io/
- Velda GitHub: https://github.com/velda-io/velda
- Velda Discord Support: https://discord.gg/M49NmjPU
- VESSL AI: https://cloud.vessl.ai
- VESSL AI Docs: https://docs.cloud.vessl.ai
- VESSL AI Demo: https://app.arcade.software/share/Rkd7eFmlwkDHboyv7RUV
- VESSL AI Hackathon Readme: https://github.com/vessl-ai/physical-ai-hackathon-demo

### Data & Models
- World Intelligence HuggingFace: https://huggingface.co/WorldIntelligence
- Community Dataset (water pouring data): https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v3/tree/main/LeRobot-worldwide-hackathon/91-AM-PM-pouring-liquid
- SoloHack Pouring Repo: https://huggingface.co/SoloHack/pouring
- Team Datasets (Jon's): https://huggingface.co/jjschong/datasets

### Unitree G1
- LeRobot Docs for Unitree: https://huggingface.co/docs/lerobot/en/unitree_g1
- Unitree SDK Python: https://github.com/unitreerobotics/unitree_sdk2_python
- Unitree G1 Replay: https://github.com/GetSoloTech/unitree-g1-replay
- Unitree Isaac Lab Sim: https://github.com/unitreerobotics/unitree_sim_isaaclab
- Unitree GROOT WBC: https://github.com/NVlabs/GR00T-WholeBodyControl
- Unitree IL LeRobot: https://github.com/unitreerobotics/unitree_IL_lerobot

### OpenDroid / Realman
- Quick Start Guide: https://develop.realman-robotics.com/en/robot/quickUseManual/
- Teaching & Programming Guide: https://develop.realman-robotics.com/en/robot/teachingPendant/armTeching/
- OpenDroids GitHub: https://github.com/Open-Droids-robot

### Booster
- Official SDK Docs: https://www.booster.tech/open-source/

### VLA Papers & Code
- ACT/ALOHA Paper: https://tonyzhaozh.github.io/aloha/
- ACT Code: https://github.com/tonyzhaozh/act
- ACT LeRobot: https://huggingface.co/docs/lerobot/en/act
- Pi0 Paper: https://www.pi.website/blog/pi0
- Pi0 Code: https://github.com/Physical-Intelligence/openpi
- Pi0 LeRobot: https://huggingface.co/docs/lerobot/en/pi0
- Pi0Fast Paper: https://www.pi.website/research/fast
- Pi0Fast LeRobot: https://huggingface.co/docs/lerobot/en/pi0fast
- Pi05 Paper: https://www.pi.website/blog/pi05
- Pi05 LeRobot: https://huggingface.co/docs/lerobot/en/pi05
- NVIDIA GROOT N1.5 Paper: https://research.nvidia.com/labs/gear/gr00t-n1_5/
- NVIDIA GROOT Code: https://github.com/NVIDIA/Isaac-GR00T
- GROOT LeRobot: https://huggingface.co/docs/lerobot/en/groot
- X-VLA Paper: https://arxiv.org/pdf/2510.10274
- X-VLA LeRobot: https://huggingface.co/docs/lerobot/en/xvla
- SmolVLA Blog: https://huggingface.co/blog/smolvla
- Diffusion Policy: https://diffusion-policy.cs.columbia.edu/
- Diffusion Policy GitHub: https://github.com/real-stanford/diffusion_policy
- VLA Survey: https://vla-survey.github.io/

### Other Resources
- Mobile ALOHA: https://mobile-aloha.github.io/
- Noisebridge (SF robotics community): (Discord)
- SO-101 Assembly Video (helpful): https://www.youtube.com/watch?v=70GuJf2jbYk

### Team Materials
- Team Shared GDrive: https://drive.google.com/drive/folders/14MnFUaW8-HeQNgBXzABeMsA-9hhZTRHX
- Project Submission (Devspot): https://devspot.app/projects/884
- Submission Slideshow: https://docs.google.com/presentation/d/1aEaip9ZFcIVnPLBpbkDN_S2j6nFPJh4cIznNbAK1mvI/edit
- Technical Reference Doc: https://docs.google.com/document/d/1VvU5bHygaDBd2SsUidZsjWfZ0BKCUY142_aQGMIwkXg/edit
- Velda GPU Access Doc: https://docs.google.com/document/d/1L0hfiASLUWGS1LmewgAlB9Ibko-pS0WcuX0NET07rJo/edit

### Social
- Dhruv Diddi Twitter: https://x.com/DhruvDiddi/status/2016631851025383518
- Devinder Sodhi LinkedIn (robot setup): https://www.linkedin.com/posts/devindersodhi_5v-supply-32-12v-supply-19-waveshare-activity-7423352456144805888-ckOI
- LinkedIn Announcement: https://www.linkedin.com/feed/update/urn:li:activity:7422399185393573888/

---

## Personal Recollection

### Pre-Event

The event was super exciting, the most exciting one I've done yet—starting with the team matching 3-4 days prior (they did this purportedly to know who to assign robots to but also probably to determine acceptance/waitlist to the event). They also didn't release the full count of robots until asked—and it changed in the days leading up to the event, so they were probably still firming that up.

I wanted to get at least SOME robotics experience if I could—so I looked into where I could operate one of the robots ClaudeAI recommended we try (it zeroed in on an arm like the SO101). My first search saw it was too late to get one from Amazon (or anywhere else, even emailed a company about expediting that turned out to be in SF—but they didn't get back to me until after I'd hit purchase on Amazon). I also asked in Noisebridge discord about borrowing or using one—they have an active robotics community worth checking out more. One guy offered for me to pick his up but again—after I found one on Amazon. I also messaged a guy (Ismael) on FB marketplace that turned out to be a co-host of the event!

Anyways, I bought an SO101 to arrive the day before the event—for around $350 + printed parts + clamps and a webcam Claude said I'd need—total around $450ish. But resale value looks good on them on ebay, and so far I don't regret purchasing—at the time we were still worried they wouldn't have enough robots to go around at the event too.

Anyways, it took maybe 6-7 hours to build the bot the day/night before—it only came with a QR code to the repo (lerobot repo I think)—and I still have yet to find an all-in-one official manual with instructions good for robotics newbies. Things like which way to attach the flywheels on the servos were found more easily by zooming in on a video installation than by googling.

### During the Event

We had a team of 5. I was the only guy above 30. One guy was shy enough he mostly wandered off and wasn't around enough by our team submission to include (so we submitted as 4). Another guy (who trained our model overnight) claimed food poisoning and didn't show on day 2 either (Sunday).

I felt like I mostly contributed (though not on the model training—I do still intend to go back through the model training to follow his footsteps) by bringing the robot, and by getting us maybe half of the way to recording the leader-follower training data we'd use (which might have been of limited value anyways since we only had 25ish runs added to idk how many piggybacked episodes from the previously existing data set—which I insisted we look for though, and which might have been a large part of our success—I got as far on my laptop as successfully setting up the leader-follower teleoperation (with good calibration we had to troubleshoot a bit), but the camera backend wasn't working after an hour or 2 of trying to hack it, so Siddha (a freshman? at NYU who flew in just for this!) installed it all on his Macbook and surpassed where I'd gotten to—with tips from me along the way—and they still needed my Win laptop to troubleshoot the motor IDs—which randomly got reassigned at least twice (first time I may have not assigned them correct, idk). We also had random-seeming port issues that may have been due to having too many USBs on one hub (we eventually chained a usb hub to another that was video enabled, which we had 2 of, but one had a frayed wire—also basically 3 teams were all working off the same wall outlet on multiple power adapters chained!)

Anyways, he got the camera working so the model would work 'with vision'... we had 2 cameras, one on the claw and one on a 'tripod' we made by putting our full water bottles into empty boxes coke cans come in. We rushed to get in as many RL episodes (of the leader arm controlling the follower to pour water) and spent the last hour or so to get 25 episodes (we were told to aim for 50).

Udit (the team 'organizer' who invited my 'team' of me and Kush to join them) trained the model overnight, and we ran it in the morning. At first it didn't seem to do much—it seemed to activate but only random small movements with the arm in place. BUT after we got, I think, the cameras working? or something—all of a sudden it clicked, and it grabbed the bottle as we had and slowly (often shakily) poured it into the cup!

**SUCH an OMG/AHA moment. SUPER feeling.**

Many times it would be shaky and stop or oscillate right before pouring. But after the first few runs it seemed to be getting the pour at least most or all in the cup 70-80% of the time!

Suffice to say it felt pretty cool to have done for a team with basically nil previous experience with robotics!

### Performance Degradation

By the evening it had stopped working consistently—someone had bumped our hacked soda-box tripod, and we heard the models can be super sensitive to camera angle/positioning/other conditions. By the time we were judging it was barely pouring 1/2 the time, if that. It got super jerky for awhile and would suddenly BITE the cup/snap like a bird. We probably should have spent more of the morning/afternoon testing and making sure things were consistent but we were mostly flying blind on many things and had to resort to asking AI and organizers what to do.

I also got the cameras working with teleoperation on my Win laptop, but not sure if I got recording to work. Perhaps I can figure it out myself at home—the robot only takes 30 min to set up or so.

### Results

We didn't win—the top 7 teams (out of a few hundred people there) presented, and top 3 won $3k/$2k/$1k with a couple other smaller prizes. Winning team had a guy with years of robotics experience—they strictly controlled their training environment (with tote bags on either side to make a mini-'room') and even trained the model 10-20 times with interrupting it, pulling the target cup to the side while the leader operator still poured in the right place.

I was amazed to meet everyone and take in the energy.

There's a follow-up event by same organizer (SoloTech) in March; it's on my radar to go to—though now OpenClaw just came out and I'm also excited about that.

---

## Event Overview

### Format
- 48-hour hackathon with ~60 teams
- ~900+ builders signed up, ~300 selected
- Free to attend
- 24h venue access (closes 12 AM Sunday, reopens 8 AM)
- Robot access via scheduled time slots

### Hosts
- **Presented by**: Solo Tech
- **Co-Hosted by**: World Intelligence, Oli Robotics, KikiTora
- **Organizers**: Dhruv Diddi, Grace Zhang, Devinder Sodhi, Astrid Wilde, Ismail Karankin, Anna Shive

### Sponsors (Backers)
- Decart
- Rerun
- Virtuals Protocol
- Halfit
- Markov Robotics
- Midcentury
- Protocol Labs

---

## Judging Protocol

**This is a benchmark event. Not a demo competition.**

### Point Distribution
| Category | Points | Description |
|----------|--------|-------------|
| Performance | 50 | Did it work? How well? |
| Innovation | 30 | Novel approaches that advance the field |
| Generalization | 20 | Does it work beyond demo conditions? |

### Performance Scoring (50 pts)
- 45-50: Exceptional — Top-tier metrics, consistent success
- 35-44: Strong — Reliable completion, competitive
- 25-34: Functional — Task completed, needs improvement
- 15-24: Partial — Some success, significant failures
- 0-14: Attempt — Engaged but limited success

### Innovation Scoring (30 pts)
- 27-30: Breakthrough — Novel architecture, clear advantages
- 21-26: Significant — Meaningful improvement on existing
- 15-20: Incremental — Competent with minor innovations
- 8-14: Standard — Known approaches, well-executed
- 0-7: Derivative — No technical differentiation

### Generalization Scoring (20 pts)
- 18-20: Cross-platform — Multiple robot platforms
- 14-17: Cross-task — Multiple tasks, same track
- 10-13: Robust — Lighting/position variations
- 5-9: Specific — Demo conditions only
- 0-4: Brittle — Precise setup required

### Bonus Points
- +3: Same model on 2+ robot platforms
- +2: Same approach across different tracks

### Demo Protocol (20 minutes total)
1. Setup: 5 min (calibration, no scoring)
2. Demo: 10 min (scored attempts)
3. Q&A: 5 min (judge questions)

### Attempt Rules
- Declare attempt count before starting (min 3, max 10)
- All declared attempts count toward Success Rate
- Cannot add attempts after declaration
- Aborted attempts = failures (unless hardware malfunction)

### Hardware Failure Policy
- Motor fault / communication loss = 1 retry allowed
- Maximum 2 hardware failure retries per demo
- Software/control failures: no retry
- Judge panel determines failure type. Decision is final.

### What to Prepare
- Working demo on at least one physical robot
- Technical summary (1-page max): approach, architecture, innovations
- Code repository for post-demo verification

---

## Track Specifications

### Track 01: Puzzle & Shape Insertion

**Scoring Metrics**
| Metric | Weight | Formula | Target |
|--------|--------|---------|--------|
| Success Rate | 70% | Shapes placed / Attempts | > 90% |
| Cycle Time | 30% | Avg time per success | < 8s |

**Task Setup**
- Objects: 5 shapes (circle, square, triangle, pentagon, star)
- Starting Position: Shapes in tray, 20cm from board
- Success Condition: Shape fully seated, stable 2s

### Track 02: Plugging In Chargers

**Scoring Metrics**
| Metric | Weight | Formula | Target |
|--------|--------|---------|--------|
| Success Rate | 60% | Insertions / Attempts | > 85% |
| Alignment Error | 40% | Deviation from axis | < 2mm |

**Task Setup**
- Objects: USB-C charger, standard outlet
- Starting Position: Charger grasped, 15cm from outlet
- Success Condition: Fully inserted, charging indicator on

### Track 03: Pouring Liquid

**Scoring Metrics**
| Metric | Weight | Formula | Target |
|--------|--------|---------|--------|
| Volume Accuracy | 50% | Actual / Target volume | > 95% |
| Spill Rate | 50% | Liquid outside target | < 5ml |

**Task Setup**
- Objects: 500ml source (400ml filled), 300ml target
- Starting Position: Source grasped, 25cm from target
- Success Condition: Pour 200ml (±10ml), spill < 50ml

---

## Robots Available

| Robot | Type | DOF | Framework | Difficulty | Notes |
|-------|------|-----|-----------|------------|-------|
| **SO-101** | Tabletop arm | 6 + gripper | LeRobot native | Easy | Recommended starting point |
| **Koch** | Tabletop arm | 6 + gripper | LeRobot native | Easy | Similar to SO-101 |
| **LeKiwi** | Mobile + arm | 6 + gripper + 3 wheels | LeRobot native | Medium | SO-101 on omnidirectional base |
| **OpenDroids R1D2** | Mobile manipulator | Single arm | ROS2 | Medium | $18,500 retail |
| **OpenDroids R2D3** | Mobile manipulator | Dual arm | ROS2 | Medium | $55,000 retail |
| **Unitree G1** | Humanoid | 23-29 | LeRobot (via unitree_IL_lerobot) | Hard | Requires XR teleoperation |
| **Open Duck Mini** | Quadruped | 12 | Unknown | Medium | No arms — locomotion only |
| **Booster** | Unknown | Unknown | Custom SDK | Medium | RL training, Isaac Lab Sim |

**Approved Platforms for Competition:**
- Unitree G1
- Open Droid R1D2/R2D3
- Open Duck Mini
- LeRobot SO-101
- LeKiwi
- Custom hardware (if approved 48+ hours before event)

---

## VLA Models

### Policy Architectures (Quick Reference)
| Policy | Parameters | Speed | Best For |
|--------|------------|-------|----------|
| **ACT** | ~10M | ~30 Hz | Fast baseline, start here |
| **Diffusion Policy** | ~100M | ~10 Hz | Complex multimodal tasks |
| **SmolVLA** | 450M | ~10 Hz | Language-conditioned tasks |
| **OpenVLA** | 7B | ~2-4 Hz | Maximum generalization |

### Models with LeRobot Integration
- ACT
- Pi0, Pi0Fast, Pi05
- NVIDIA GROOT N1.5
- X-VLA
- SmolVLA

### State-of-Art VLAs (Jan 2026)
| Model | Source | Status |
|-------|--------|--------|
| π0 / π0-FAST | Physical Intelligence | SOTA generalist, partially open |
| GR00T N1 | NVIDIA | SOTA humanoid, recently open |
| Helix | Figure AI | SOTA dual-system, closed |
| SmolVLA | Hugging Face | Open, compact, hackathon-friendly |
| Gemini Robotics | Google DeepMind | Closed |

---

## GPU & Cloud Resources

### Velda
- URL: https://physical-ai.velda.cloud
- GPUs Available: NVIDIA L4, A100-1, A100-2
- Access to multiple L4s available
- Immediately ready dev environment
- **Note**: Site shut down ~1 week after event

**Pool Specs:**
| Pool | CPU | Memory | Scratch /tmp | Limit per team |
|------|-----|--------|--------------|----------------|
| gpu-a100-1 | 12 | 85G | 375G | 1 (1 preemptible) |
| gpu-a100-2 | 24 | 170G | 750G | 1 (1 preemptible) |
| gpu-l4-1 | 8 | 32G | 375G | 4 (8 preemptible) |

**Usage:**
```bash
vrun -P gpu-a100-1 uv run solo robo --train
```

### VESSL AI
- URL: https://cloud.vessl.ai
- GPUs: H100×1/2/4, A100 (80GB)×1/2/4/8
- H100s opened after 3 PM
- A100s (80GB) available immediately

---

## Sponsors & Provided Resources

### Solo Tech
- Base VLM and VLA models + fine-tuning workflow
- Products: Solo Gym (training), Solo Hub (model hosting)
- Models offered: OpenVLA, RT-2, PaliGemma, Florence-2
- Claimed performance: Sub-10ms inference latency

### World Intelligence
- 50+ hours multimodal egocentric data
- Includes: 2D video, depth, IMU, audio
- Collected from same task families used in the hack

### KikiTora
- 40+ hours human pose data for locomotion
- Includes: RGB, depth, camera intrinsics/extrinsics, segmentation masks, COCO keypoints/joints (2D pixel + 3D world space)
- Use case: Training locomotion policies (humanoid/quadruped)

### Oli Robotics
- Imitation learning robotics data and tooling
- Demonstrations related to automated coffee-making
- On-site mentorship during event

---

## Technical Reference Guide

(Copied from official hackathon reference doc)

### Note
Motors are already set up, you can start with the calibration and teleop

### SO-101 & Koch Resources
- Solo-CLI GitHub & Installation Guide
- Official SO-101 Docs
- Official Koch Docs
- LeRobot GitHub

### LeKiwi Resources
- Official LeKiwi Docs
- LeRobot GitHub & Installation
- Additional Docs for Assembly & Setup (Seeed Wiki)

### OpenDroid (Realman R1D2) Resources
- Solo-CLI GitHub & Installation Guide
- Quick Start & Hardware Preparation Guide
- Robotic Arm Teaching & Programming Guide

### Unitree G1 Resources
- LeRobot Docs for Unitree
- Official Unitree SDK Documentation
- Action & Replay Implementation
- Unitree IsaacLab Sim
- Unitree GROOT Wholebody Control

### Booster Resources
- Official SDK Docs

---

## Luma Event Blasts

### Jan 27, 11:18 AM (Grace Zhang)
Team Formation announcement. Form: https://physicalaihack.com/intake

### Jan 28, 2:15 PM (Dhruv Diddi)
Info Session announcement. 900+ builders signed up, only 300 selected.

### Jan 29, 3:27 PM (Dhruv Diddi)
- "Hacker" access started granting
- "Spectator [Sunday 3-6pm]" access for non-selected
- Non-selected teams get priority for March event
- Robot list shared

### Jan 30, 5:41 PM (Dhruv Diddi)
Logistics session updated to 7:30pm. Technical docs link shared.

### Jan 30, 7:28 PM (Dhruv Diddi)
- Logistics Session: meet.google.com/zkt-zyjs-ueg
- Robot Reservation: https://forms.gle/tKR8pNUdC7PJZRuK9

### Jan 31, 6:59 AM (Dhruv Diddi)
"Welcome to the largest Physical AI hackathon with the largest collection of open source robotics in the planet!"

### Jan 31, 10:02 AM (Dhruv Diddi)
Due to overwhelming registrations, attendance limited. Only "Hacker" tag allowed. Open demos Sunday 3pm-6pm for Spectators.

### Jan 31, 11:02 AM (Grace Zhang)
World Intelligence datasets: https://huggingface.co/WorldIntelligence
"Feel free to come to the kitchen to collect more egocentric+tactile data with us."

### Feb 1, 9:27 AM (Dhruv Diddi)
- Parking limited due to Farmers Market at Fort Mason
- Submissions due at 1pm
- Record 2-3 min video
- Judging science fair style, top 7 present on stage

### Feb 1, 12:11 PM (Dhruv Diddi)
Final Submission Requirements:
1. Project Demo Video (3 min max)
2. Project Slides (PDF format) with: Team Name & Members, Project Idea, Key Problems, Main Insights, Next Steps

---

## Velda Documentation

### Overview
Velda is a cloud development platform designed to make cloud-scaling simple and dev-ops free.

### Usage for Hackathon
1. Upload data to HuggingFace
2. Visit https://physical-ai.velda.cloud
3. Register account
4. Click connect button on solo-cli
5. VS-code opens in browser
6. Run: `vrun -P gpu-a100-1 uv run solo robo --train`
7. Keep page & laptop open while training

### Important Notes
- 1 account per team for hackathon access
- 30GB root disk limit
- Use /tmp for large data (not persisted, local disk)
- GPUs hosted at Google Cloud us-central1-b
- Credits provided by SoloTech
- Site shut down ~1 week after event

---

## Team Submission Materials

### Team
- Jon Chong
- Sota Miyajima
- Siddha Kanthi
- Udit Karthikeyan

(5th member not included in final submission)

### Project: SO101 Water Pouring Demo

### Submission Links
- Devspot Project: https://devspot.app/projects/884
- Slideshow: https://docs.google.com/presentation/d/1aEaip9ZFcIVnPLBpbkDN_S2j6nFPJh4cIznNbAK1mvI/edit

### Data
- Training repo: https://huggingface.co/SoloHack/pouring
- Base dataset (piggybacked): https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v3/tree/main/LeRobot-worldwide-hackathon/91-AM-PM-pouring-liquid
- Team datasets: https://huggingface.co/jjschong/datasets
- Team GDrive: https://drive.google.com/drive/folders/14MnFUaW8-HeQNgBXzABeMsA-9hhZTRHX

### Technical Approach
- Robot: SO-101 (purchased, ~$450 total with parts)
- Framework: LeRobot
- Task: Water pouring (Track 03)
- Episodes recorded: ~25 (target was 50)
- Cameras: 2 (claw-mounted + tripod made from water bottles in Coke boxes)
- Training: Overnight on teammate's laptop
- Peak performance: 70-80% success rate
- Demo day performance: ~50% or less (camera angle shifted)

### Key Learnings
- Models extremely sensitive to camera positioning
- Piggybacking on existing datasets valuable
- USB hub chaining can cause port issues
- Motor IDs can randomly reassign
- Environment control (winning team used tote bags) matters
- Perturbation training (interrupting model during training) improves robustness

---

## Event Timeline

### Saturday, January 31
- 9:00 AM: Doors open
- 10:00 AM: Kickoff
- 11:00 AM: Hardware checkout, task stations open
- 12:00 PM: Building begins

### Overnight
- Venue stays open 24h
- "3am is when the real work happens"

### Sunday, February 1
- 12:00 PM: Code freeze, submission deadline
- 1:00 PM: Demos begin
- 4:00 PM: Awards ceremony
- 5:00 PM: Wrap

### Prizes
- 1st: $3,000
- 2nd: $2,000
- 3rd: $1,000
- Additional smaller prizes

---

## Allowed vs Prohibited

### Allowed
- Pre-trained foundation models (vision, language, VLA)
- Public robotics datasets
- Partner-provided data (Solo Tech, World Intelligence, Oli Robotics)

### Prohibited
- Pre-training on exact judging task setups
- Non-disclosed proprietary data

Teams must declare all pre-training data sources in technical summary.

---

*Archive compiled February 2026*
